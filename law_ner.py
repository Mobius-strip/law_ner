# -*- coding: utf-8 -*-
"""LAW_NER.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zN2DFU6LZFJZHmSunKYA8a_SS14sYwmE
"""

# CUSTOM NER FOR LAW ENTITIES IN TEXT

# Read the Example1.txt

example1 = "/content/tags.txt"
file1 = open(example1, "r")# the cumilative tag file of all the tag files is read

# Read the file

FileContent = file1.read()
c=FileContent.split(", ")
c=list(set(c))#this is done to get unique list of all the tags 
c



def create_training_data(data,type):
  patterns=[]
  for item in data:
    pattern={
        "label":type,
        "pattern":item
    }
    patterns.append(pattern)
  return patterns

patterns= create_training_data(c,"LAW_ENT")

print(patterns)

import spacy

from spacy.pipeline import EntityRuler

from spacy.lang.en import English

def generaterules(patterns):
  nlp=English()# making the language object 
  ruler= EntityRuler(nlp)# creating a set of patterns with corresponding labels
  ruler.add_patterns(patterns)
  nlp.add_pipe(ruler)#adding it to the new pipeline 
  nlp.to_disk("law_ner")

generaterules(patterns)

nlp=spacy.load("law_ner")# this creates the new labels

example2 = "/content/statement.txt"# opening the combined text files 
file2 = open(example2, "r", encoding='latin-1')

File2Content = file2.read( )

chunks= File2Content.split("\n\n\n")#here one chunk in chunks is a statement case, so we have split the file into multiple cases

def test_model(model,text):
  doc=nlp(text)# tokensing the given text 
  results=[]
  entities=[]
  for ent in doc.ents:
    entities.append((ent.start_char,ent.end_char,ent.label_))# making it in a json format to be precessed by spacy 
  if len(entities)>0:
    results=[text,{"entities": entities}] # only adding the result if there are entities present in the text of the given segment 
  return results

TRAIN_DATA=[] # preparing the training data 
for chunk in chunks:# iterating over 
  
  segments=chunk.split("\n\n")# creating segements in each case 
  hits=[]
  for segment in segments:
    segment=segment.strip()# cleaning the segment 
    segment=segment.replace("\n"," ")# cleaning the segment
    results=test_model(nlp,segment)# getting the entities in the json formant 
    #print(results)
    if results!=[]:#only if entites are present in the given segment, that segment is added
      TRAIN_DATA.append(results)


    
#i

TRAIN_DATA[1]

import json

def save_file(file,data):# function to save the json file 
  with open(file,"w",encoding="utf-8") as f:
    json.dump(data,f,indent=4)

save_file("training_data.json",TRAIN_DATA)

print(len(TRAIN_DATA))

def load_file(file):# function to load the file 
  with open(file,"r",encoding="utf-8") as f:
    data=json.load(f)
  return(data)

TRAIN_DATA=load_file("/content/training_data.json")

import random

def train_spacy(data,iterations):# more the number of iteration, less would be the losses but more training time 
    TRAIN_DATA = data
    L=[]
    nlp = spacy.blank('en') # creating a blank language ckass 
    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner, last=True)
       

   # adding the labels
    for _, annotations in TRAIN_DATA:
         for ent in annotations.get('entities'):
            ner.add_label(ent[2])

    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
    with nlp.disable_pipes(*other_pipes):  # disabling other pipelines since our function only involves ner 
        optimizer = nlp.begin_training()
        for itn in range(iterations):
            print("Statring iteration " + str(itn))
            random.shuffle(TRAIN_DATA)
            losses = {}
            for text, annotations in TRAIN_DATA:
                nlp.update(
                    [text],  # batch of texts
                    [annotations],  # batch of annotations
                    drop=0.2,  # dropout - make it harder to memorise data
                    sgd=optimizer,  # callable to update weights
                    losses=losses)
            print(losses)
            L.append(losses)
            
    return nlp

import spacy

nlp=train_spacy(TRAIN_DATA,30)

nlp.to_disk("law_ner_model")#saving the model

example3 = "/content/TEST.txt" # opening the test file 
file3 = open(example3, "r", encoding='latin-1')
File3Content = file3.read( )

case_statements= File3Content.split("\n\n\n")

from google.colab import files



i = 100
for case_statement in case_statements:# iterating through the case statements 
      case_statement=case_statement.replace("\n"," ")
      doc=nlp(case_statement)#tokenizing the doc statement 
      list=[]
      for ent in doc.ents:
        if ent.text not in list:
          list.append(ent.text)#gettin the Law identities in the case and appending it to list
      s=", "
      s=s.join(list)
      f = open("case{0}.txt".format(i), "w") # writing all the found entites on the output file 
      f.write(s)
      f.close()
      files.download("case{0}.txt".format(i))#autodownloading the files 
      i=i+1